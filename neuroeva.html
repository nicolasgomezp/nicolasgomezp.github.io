<!DOCTYPE html>
<html>
<body>
    <div style="position: relative;">
        <video id="webcam" width="640" height="480" autoplay style="position: relative; z-index: 1;"></video>
        <textarea id="inputText" style="position: absolute; top: 10px; left: 10px; color: black; background-color: rgba(255, 255, 255, 0.7); padding: 5px; z-index: 3; width: 300px; height: 50px;"></textarea>
        <div id="output" style="position: absolute; top: 70px; left: 10px; color: white; background-color: rgba(0, 0, 0, 0.5); padding: 5px; z-index: 2;"></div>
    </div>
    <canvas id="canvas" width="640" height="480" style="display:none;"></canvas>
    <button id="analyzeDetailedButton">Detailed Analysis</button>
    <button id="analyzeShortButton">Short Description</button>
    <button id="analyzeResolveButton">Normal</button>
    <button id="switchCameraButton">Switch Camera</button>
    <button id="analyzeSingleButton" style="background-color: red; color: white;">Single Analysis</button>
<script src="https://js.puter.com/v2/"></script>
<script>
    const video = document.getElementById('webcam');
    const canvas = document.getElementById('canvas');
    const outputDiv = document.getElementById('output');
    const inputText = document.getElementById('inputText');
    const analyzeDetailedButton = document.getElementById('analyzeDetailedButton');
    const analyzeShortButton = document.getElementById('analyzeShortButton');
    const analyzeResolveButton = document.getElementById('analyzeResolveButton');
    const toggleContinuousButton = document.getElementById('toggleContinuousButton');
    const switchCameraButton = document.getElementById('switchCameraButton');
    const analyzeSingleButton = document.getElementById('analyzeSingleButton');
    const context = canvas.getContext('2d');

    let currentStream = null;
    let facingMode = 'environment';
    let isSpeaking = false;
    let isListening = false;
    let continuousAnalysisInterval;
    let selectedAnalysisType = null;
    let typingSpeed = 20;
    let recognition;

    async function startWebcam() {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({
                video: { facingMode: facingMode }
            });
            handleStream(stream);
        } catch (err) {
            console.error("Error accessing webcam:", err);
            outputDiv.textContent = "Error accessing webcam. Please check permissions.";
        }
    }

    function handleStream(stream) {
        if (currentStream) {
            currentStream.getTracks().forEach(track => track.stop());
        }
        video.srcObject = stream;
        currentStream = stream;
    }

    function captureImage() {
        context.drawImage(video, 0, 0, 640, 480);
        const imageDataURL = canvas.toDataURL('image/jpeg');
        return imageDataURL;
    }

    function speak(text) {
        return new Promise((resolve, reject) => {
            if (isSpeaking) {
                window.speechSynthesis.cancel();
            }

            const utterance = new SpeechSynthesisUtterance(text);

            utterance.voice = window.speechSynthesis.getVoices().find(voice => voice.lang === 'es-ES' || voice.lang === 'es');
            if (!utterance.voice) {
                utterance.voice = window.speechSynthesis.getVoices().find(voice => voice.lang === 'en-US' || voice.lang === 'en');
                if (!utterance.voice) {
                    console.warn("No suitable voice found. Using default.");
                }
            }
            utterance.pitch = 1;
            utterance.rate = 1;
            utterance.volume = 1;

            utterance.onstart = () => {
                isSpeaking = true;
                stopRecognition();
            };

            utterance.onend = () => {
                isSpeaking = false;
                startRecognition();
                resolve();
            };

            utterance.onerror = () => {
                isSpeaking = false;
                startRecognition();
                console.error("Speech synthesis error.");
                reject("Speech synthesis error.");
            };

            try {
              window.speechSynthesis.speak(utterance);
            } catch (error) {
              console.error("Error calling speechSynthesis.speak:", error);
              reject(error);
            }
        });
    }

    async function typeWriter(text, element) {
        element.textContent = "";
        let i = 0;
        return new Promise((resolve) => {
            function next() {
                if (i < text.length) {
                    element.textContent += text.charAt(i);
                    i++;
                    setTimeout(next, typingSpeed);
                } else {
                    resolve();
                }
            }
            next();
        });
    }

    async function analyzeImage(prompt) {
        if (isSpeaking) {
            return;
        }

        outputDiv.textContent = "Analizando...";
        try {
            const imageBase64 = captureImage();
            const userText = inputText.value;
            const combinedPrompt = prompt + " Adicionalmente, considera el siguiente contexto proporcionado por el usuario: " + userText;

            console.log("Combined Prompt:", combinedPrompt);

            const response = await puter.ai.chat(
                combinedPrompt,
                imageBase64
            );

            await typeWriter(response, outputDiv);

            await speak(response);
        } catch (error) {
            console.error("", error);
            await typeWriter("", outputDiv);
            await speak("");
        }
    }

    function startContinuousAnalysis() {
        if (selectedAnalysisType) {
            continuousAnalysisInterval = setInterval(async () => {
                analyzeImage(selectedAnalysisType.prompt);
            }, 2000);
        }
    }

    function stopContinuousAnalysis() {
        clearInterval(continuousAnalysisInterval);
    }

    function toggleContinuousAnalysis() {
        if (continuousAnalysisInterval) {
            stopContinuousAnalysis();
            toggleContinuousButton.textContent = "Start Continuous";
        } else {
            startContinuousAnalysis();
            toggleContinuousButton.textContent = "Stop Continuous";
        }
    }

    function selectAnalysisType(button, prompt) {
        analyzeDetailedButton.style.backgroundColor = '';
        analyzeShortButton.style.backgroundColor = '';
        analyzeResolveButton.style.backgroundColor = '';
        analyzeSingleButton.style.backgroundColor = '';

        button.style.backgroundColor = 'lightgreen';

        selectedAnalysisType = { button: button, prompt: prompt };
        stopContinuousAnalysis();
        toggleContinuousButton.textContent = "Start Continuous";
    }

    async function switchCamera() {
        facingMode = (facingMode === 'user') ? 'environment' : 'user';
        await startWebcam();
    }

    function startRecognition() {
        if ('webkitSpeechRecognition' in window && !isSpeaking) {
            recognition = new webkitSpeechRecognition();
            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.lang = 'es-ES';

            recognition.onstart = () => {
                isListening = true;
                console.log("Voice recognition started");
            };

            recognition.onresult = (event) => {
                let final_transcript = '';
                let interim_transcript = '';

                for (let i = event.resultIndex; i < event.results.length; ++i) {
                    if (event.results[i].isFinal) {
                        final_transcript += event.results[i][0].transcript;
                    } else {
                        interim_transcript += event.results[i][0].transcript;
                    }
                }
                inputText.value = final_transcript || interim_transcript;
            };

            recognition.onerror = (event) => {
                console.error("Recognition error:", event.error);
            };

            recognition.onend = () => {
                isListening = false;
                console.log("Voice recognition ended");
                if (!isSpeaking) {
                    recognition.start();
                }
            };

            recognition.start();

        } else {
            outputDiv.textContent = "Speech recognition not supported in this browser.";
        }
    }

    function stopRecognition() {
        if (recognition) {
            recognition.stop();
            isListening = false;
        }
    }

    analyzeDetailedButton.addEventListener('click', () => selectAnalysisType(analyzeDetailedButton, "Describe la escena detalladamente."));
    analyzeShortButton.addEventListener('click', () => selectAnalysisType(analyzeShortButton, "Que ves? Maximo 3 palabras, puedes usar menos(1, 2), usa lenguaje natural."));
    analyzeResolveButton.addEventListener('click', () => selectAnalysisType(analyzeResolveButton, "Usa lenguaje natural"));
    switchCameraButton.addEventListener('click', switchCamera);
    analyzeSingleButton.addEventListener('click', async () => {
        if (selectedAnalysisType) {
            await analyzeImage(selectedAnalysisType.prompt);
        } else {
            outputDiv.textContent = "Por favor, selecciona un tipo de an√°lisis primero.";
        }
    });

    startWebcam();
    startRecognition();
</script>
</body>
</html>
