<!DOCTYPE html>
<html>
<head>
    <title>Webcam Analysis</title>
</head>
<body>
    <div style="position: relative;">
        <video id="webcam" width="640" height="480" autoplay style="position: relative; z-index: 1;"></video>
        <textarea id="inputText" style="position: absolute; top: 10px; left: 10px; color: black; background-color: rgba(255, 255, 255, 0.7); padding: 5px; z-index: 3; width: 300px; height: 50px;"></textarea>
        <div id="output" style="position: absolute; top: 70px; left: 10px; color: white; background-color: rgba(0, 0, 0, 0.5); padding: 5px; z-index: 2;"></div>
    </div>
    <canvas id="canvas" width="640" height="480" style="display:none;"></canvas>
    <button id="analyzeDetailedButton">Detailed Analysis</button>
    <button id="analyzeShortButton">Short Description</button>
    <button id="analyzeResolveButton">Normal</button>
    <button id="switchCameraButton">Switch Camera</button>
    <button id="analyzeSingleButton" style="background-color: red; color: white;">Single Analysis</button>

<script src="https://js.puter.com/v2/"></script>
<script>
    const video = document.getElementById('webcam');
    const canvas = document.getElementById('canvas');
    const outputDiv = document.getElementById('output');
    const inputText = document.getElementById('inputText');
    const analyzeDetailedButton = document.getElementById('analyzeDetailedButton');
    const analyzeShortButton = document.getElementById('analyzeShortButton');
    const analyzeResolveButton = document.getElementById('analyzeResolveButton');
    const switchCameraButton = document.getElementById('switchCameraButton');
    const analyzeSingleButton = document.getElementById('analyzeSingleButton');
    const context = canvas.getContext('2d');

    let currentStream = null;
    let facingMode = 'environment';
    let isSpeaking = false;
    let isListening = false;
    let continuousAnalysisInterval;
    let selectedAnalysisType = null;
    let typingSpeed = 20;
    let recognition;
    let conversationHistory = [];
    let analysisInProgress = false; // Flag to prevent overlapping analysis

    async function startWebcam() {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({
                video: { facingMode: facingMode }
            });
            handleStream(stream);
        } catch (err) {
            console.error("Error accessing webcam:", err);
            outputDiv.textContent = "Error accessing webcam. Please check permissions.";
        }
    }

    function handleStream(stream) {
        if (currentStream) {
            currentStream.getTracks().forEach(track => track.stop());
        }
        video.srcObject = stream;
        currentStream = stream;
    }

    function captureImage() {
        context.drawImage(video, 0, 0, 640, 480);
        const imageDataURL = canvas.toDataURL('image/jpeg');
        return imageDataURL;
    }

   function speak(text) {
        return new Promise((resolve, reject) => {
            if (isSpeaking) {
                window.speechSynthesis.cancel();
            }

            const utterance = new SpeechSynthesisUtterance(text);

            utterance.voice = window.speechSynthesis.getVoices().find(voice => voice.lang === 'es-ES' || voice.lang === 'es');
            if (!utterance.voice) {
                utterance.voice = window.speechSynthesis.getVoices().find(voice => voice.lang === 'en-US' || voice.lang === 'en');
                if (!utterance.voice) {
                    console.warn("No suitable voice found. Using default.");
                }
            }
            utterance.pitch = 1;
            utterance.rate = 1;
            utterance.volume = 1;

            utterance.onstart = () => {
                isSpeaking = true;
                stopRecognition();
            };

            utterance.onend = () => {
                isSpeaking = false;
                startRecognition();
                resolve();
            };

            utterance.onerror = () => {
                isSpeaking = false;
                startRecognition();
                console.error("Speech synthesis error.");
                reject("Speech synthesis error.");
            };

            try {
              window.speechSynthesis.speak(utterance);
            } catch (error) {
              console.error("Error calling speechSynthesis.speak:", error);
              reject(error);
            }
        });
    }

    async function typeWriter(text, element) {
        element.textContent = "";
        let i = 0;
        return new Promise((resolve) => {
            function next() {
                if (i < text.length) {
                    element.textContent += text.charAt(i);
                    i++;
                    setTimeout(next, typingSpeed);
                } else {
                    resolve();
                }
            }
            next();
        });
    }

    async function analyzeImage(prompt) {
         if (analysisInProgress) {
            console.log("Analysis already in progress. Skipping.");
            return;
        }

        if (isSpeaking) {
            return;
        }


        analysisInProgress = true;
        outputDiv.textContent = "Analizando...";
        try {
            const imageBase64 = captureImage();
            const userText = inputText.value;

            // Construct the combined prompt with conversation history
            let combinedPrompt = prompt + " Adicionalmente, considera el siguiente contexto proporcionado por el usuario: " + userText;

            if (conversationHistory.length > 0) {
                combinedPrompt += " Aquí está el historial de conversación anterior: " + conversationHistory.join("\n");
            }

            console.log("Combined Prompt:", combinedPrompt);

            const response = await puter.ai.chat(
                combinedPrompt,
                imageBase64
            );

            await typeWriter(response, outputDiv);

            await speak(response);

            // Add the user input and AI response to the conversation history
            conversationHistory.push("Usuario: " + userText);
            conversationHistory.push("AI: " + response);

            // Limit the history to prevent it from growing too large (optional)
            if (conversationHistory.length > 10) {
                conversationHistory = conversationHistory.slice(-10);
            }
            inputText.value = ''; // Clear the input after analysis

        } catch (error) {
            console.error("Analysis error:", error);
            await typeWriter("Error en el análisis.", outputDiv);
            await speak("Error en el análisis.");
        } finally {
            analysisInProgress = false;
        }
    }



    function selectAnalysisType(button, prompt) {
        analyzeDetailedButton.style.backgroundColor = '';
        analyzeShortButton.style.backgroundColor = '';
        analyzeResolveButton.style.backgroundColor = '';
        analyzeSingleButton.style.backgroundColor = '';

        button.style.backgroundColor = 'lightgreen';

        selectedAnalysisType = { button: button, prompt: prompt };
        stopContinuousAnalysis();
    }

    async function switchCamera() {
        facingMode = (facingMode === 'user') ? 'environment' : 'user';
        await startWebcam();
    }

    function startRecognition() {
        if ('webkitSpeechRecognition' in window && !isSpeaking) {
            recognition = new webkitSpeechRecognition();
            recognition.continuous = false; // Change to false for single phrase
            recognition.interimResults = false; // Change to false for final result only
            recognition.lang = 'es-ES';

            recognition.onstart = () => {
                isListening = true;
                console.log("Voice recognition started");
            };

            recognition.onresult = async (event) => {
                const transcript = event.results[0][0].transcript;
                inputText.value = transcript;

                if (selectedAnalysisType) {
                    await analyzeImage(selectedAnalysisType.prompt); // Trigger analysis
                } else {
                    outputDiv.textContent = "Por favor, selecciona un tipo de análisis primero.";
                }

                stopRecognition(); // Stop recognition after processing
                setTimeout(startRecognition, 1000); //Restart after 1 sec
            };


            recognition.onerror = (event) => {
                console.error("Recognition error:", event.error);
                setTimeout(startRecognition, 1000); //Restart after 1 sec
            };

            recognition.onend = () => {
                isListening = false;
                console.log("Voice recognition ended");

            };

            recognition.start();

        } else {
            outputDiv.textContent = "Speech recognition not supported in this browser.";
        }
    }

    function stopRecognition() {
        if (recognition) {
            recognition.stop();
            isListening = false;
        }
    }

    analyzeDetailedButton.addEventListener('click', () => selectAnalysisType(analyzeDetailedButton, "Describe la escena detalladamente."));
    analyzeShortButton.addEventListener('click', () => selectAnalysisType(analyzeShortButton, "Que ves? Maximo 3 palabras, puedes usar menos(1, 2), usa lenguaje natural."));
    analyzeResolveButton.addEventListener('click', () => selectAnalysisType(analyzeResolveButton, "Usa lenguaje natural"));
    switchCameraButton.addEventListener('click', switchCamera);
    analyzeSingleButton.addEventListener('click', async () => {
        if (selectedAnalysisType) {
            await analyzeImage(selectedAnalysisType.prompt);
        } else {
            outputDiv.textContent = "Por favor, selecciona un tipo de análisis primero.";
        }
    });

    startWebcam();
    startRecognition();
</script>
</body>
</html>
